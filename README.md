# DE_Project_Workflow_on_AWS

## Data Engineering:
Data engineering is the practice of designing, building, and maintaining the systems and infrastructure that enable the collection, processing, storage, and analysis of large volumes of data. It involves working with technologies such as big data frameworks, databases, and ETL tools, to ensure that data is properly managed and can be easily accessed and analyzed by data scientists and other stakeholders. Data engineers play a critical role in helping organizations derive insights from their data, and in building data-driven products and services.

## Role of AWS in DE:
AWS (Amazon Web Services) provides a range of services that are useful for data engineering, including storage, processing, analysis, and management of large volumes of data. For example, Amazon S3 provides scalable object storage for unstructured data, Amazon EMR allows for processing of large data using big data frameworks, Amazon Redshift is a fast data warehousing service, and Amazon Glue is an ETL service for building data pipelines. Additionally, AWS provides serverless query services like Amazon Athena, allowing for interactive querying of data stored in AWS. By leveraging AWS services, data engineers can efficiently and effectively handle data at scale, and build robust data architectures that meet their organization's needs.

### For overall workflow of the project... Please refer the blog link provided below 
[Workflow Of Data Engineering Project on AWS](https://de-project-workflow-aws.blogspot.com/2023/03/1.html)
